---
title: 'Predictive Analytics in-class exercise on Cancer Detection '
output:
  html_document:
    toc: yes
    toc_depth: '3'
    df_print: paged
    pdf_document:
    toc: yes
    toc_depth: 3
---
\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "C:\\Users\\rbapna\\Dropbox\\NYUPredModelingCourse\\labs\\lab8-cancerDetection Revisited")

```


```{r message=FALSE,  warning=FALSE}
# load the required libraries
library("readxl") # used to read excel files
library("dplyr") # used for data munging 
library("FNN") # used for knn regression (knn.reg function)
library("caret") # used for various predictive models
library("class") # for using confusion matrix function
library("rpart.plot") # used to plot decision tree
library("rpart")  # used for Regression tree
library("glmnet") # used for Lasso and Ridge regression
library('NeuralNetTools') # used to plot Neural Networks
library("PRROC") # top plot ROC curve
library("ROCR") # top plot lift curve
library("tidyverse")
library("xgboost")
```


# 1. Classification


## 1.1 Data loading and transformation

Please download the Breast Cancer data set from the below mentioned links  

Data: https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data

Description: https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.names

```{r }
# Load the Breast Cancer data set

cancer_data = read_csv("wdbc.data", col_names =  FALSE)

# create Y and X data frames
cancer_y = cancer_data %>% pull("X2") %>% as.factor()
# exclude X1 since its a row number
cancer_x = cancer_data %>% select(-c("X1", "X2"))
```

Create a function that normalizes columns since scale for each column might be different.

```{r }
# function to normalize data (0 to 1)
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}
```

```{r }
# Normalize x variables since they are at different scale
cancer_x_normalized <- as.data.frame(lapply(cancer_x, normalize))
```

Create Training and Testing data sets

```{r }
# 75% of the data is used for training and rest for testing
smp_size <- floor(0.75 * nrow(cancer_x_normalized))

# randomly select row numbers for training data set
train_ind <- sample(seq_len(nrow(cancer_x_normalized)), size = smp_size)

# creating test and training sets for x
cancer_x_train <- cancer_x_normalized[train_ind, ]
cancer_x_test <- cancer_x_normalized[-train_ind, ]

# creating test and training sets for y
cancer_y_train <- cancer_y[train_ind]
cancer_y_test <- cancer_y[-train_ind]

# Create an empty data frame to store results from different models
clf_results <- data.frame(matrix(ncol = 5, nrow = 0))
names(clf_results) <- c("Model", "Accuracy", "Precision", "Recall", "F1")

# Create an empty data frame to store TP, TN, FP and FN values
cost_benefit_df <- data.frame(matrix(ncol = 5, nrow = 0))
names(cost_benefit_df) <- c("Model", "TP", "FN", "FP", "TN")


```

**Cross validation**

It is a technique to use same training data but some portion of it for training and rest for validation of model. This technique reduces chances of overfitting

**Hyperparamter tuning**

We provide a list of hyperparameters to train the model. This helps in identifying best set of hyperparameters for a given model like Decision tree. **train** function in caret library automatically stores the information of the best model and its hyperparameters.



## 1.2 KNN Classification

```{r }

# Cross validation 
cross_validation <- trainControl(## 10-fold CV
                                method = "repeatedcv",
                                number = 10,
                                ## repeated three times
                                repeats = 3)
# Hyperparamter tuning
# k = number of nrearest neighbours
Param_Grid <-  expand.grid( k = 1:10)

# fit the model to training data
knn_clf_fit <- train(cancer_x_train,
                     cancer_y_train, 
                     method = "knn",
                     tuneGrid = Param_Grid,
                     trControl = cross_validation )

# check the accuracy for different models
knn_clf_fit

```

```{r }
# Plot accuracies for different k values
plot(knn_clf_fit)

# print the best model
print(knn_clf_fit$finalModel)
```

```{r }
# Predict on test data
knnPredict <- predict(knn_clf_fit, newdata = cancer_x_test) 

```

```{r }
# Print Confusion matrix, Accuracy, Sensitivity etc 
confusionMatrix(knnPredict, cancer_y_test, positive = "M")

# Add results into clf_results dataframe
x1 <- confusionMatrix(knnPredict, cancer_y_test)[["overall"]]
y1 <- confusionMatrix(knnPredict, cancer_y_test)[["byClass"]]

clf_results[nrow(clf_results) + 1,] <-  list(Model = "KNN", 
                                             Accuracy = round (x1[["Accuracy"]],3), 
                                            Precision = round (y1[["Precision"]],3), 
                                            Recall = round (y1[["Recall"]],3), 
                                            F1 = round (y1[["F1"]],3))
# Print Accuracy and F1 score

cat("Accuarcy is ", round(x1[["Accuracy"]],3), "and F1 is ", round (y1[["F1"]],3)  )

# Add results into cost_benefit_df dataframe for cost benefit analysis 
a1 <- confusionMatrix(knnPredict, cancer_y_test)

cost_benefit_df[nrow(cost_benefit_df) + 1,] <-  list(Model = "KNN", 
                                             TP = a1[["table"]][1], 
                                             FN = a1[["table"]][2], 
                                             FP = a1[["table"]][3], 
                                             TN = a1[["table"]][4])

```


## 1.3 Decision Tree Classification 

```{r }

# Cross validation
cross_validation <- trainControl(## 10-fold CV
                                method = "repeatedcv",
                                number = 10,
                                ## repeated three times
                                repeats = 3)
# Hyperparamter tuning
# maxdepth =  the maximum depth of the tree that will be created or
# the length of the longest path from the tree root to a leaf.

Param_Grid <-  expand.grid(maxdepth = 2:10)

dtree_fit <- train(cancer_x_train,
                   cancer_y_train, 
                   method = "rpart2",
                   # split - criteria to split nodes
                   parms = list(split = "gini"),
                  tuneGrid = Param_Grid,
                   trControl = cross_validation,
                  # preProc -  perform listed pre-processing to predictor dataframe
                   preProc = c("center", "scale"))

# check the accuracy for different models
dtree_fit
```

```{r }
# print the final model
dtree_fit$finalModel
```

```{r }
# Plot decision tree
prp(dtree_fit$finalModel, box.palette = "Reds", tweak = 1.2)
```

```{r }
# Predict on test data
dtree_predict <- predict(dtree_fit, newdata = cancer_x_test)
```

```{r }
# Print Confusion matrix, Accuarcy, Sensitivity etc 
confusionMatrix(dtree_predict,  cancer_y_test , positive = "M")

# Add results into clf_results dataframe
x2 <- confusionMatrix(dtree_predict,  cancer_y_test )[["overall"]]
y2 <- confusionMatrix(dtree_predict,  cancer_y_test )[["byClass"]]

clf_results[nrow(clf_results) + 1,] <-  list(Model = "Decision Tree", 
                                             Accuracy = round (x2[["Accuracy"]],3), 
                                            Precision = round (y2[["Precision"]],3), 
                                            Recall = round (y2[["Recall"]],3), 
                                            F1 = round (y2[["F1"]],3))

# Print Accuracy and F1 score

cat("Accuarcy is ", round(x2[["Accuracy"]],3), "and F1 is ", round (y2[["F1"]],3)  )

# Add results into cost_benefit_df dataframe for cost benefit analysis 
a2 <- confusionMatrix(dtree_predict,  cancer_y_test )

cost_benefit_df[nrow(cost_benefit_df) + 1,] <-  list(Model = "Decision Tree", 
                                             TP = a2[["table"]][1], 
                                             FN = a2[["table"]][2], 
                                             FP = a2[["table"]][3], 
                                             TN = a2[["table"]][4])

```

## 1.4 Logistic regression

Convert categorical outcome into numerical. 

```{r }
cancer_y_train_l <- ifelse(cancer_y_train =="M", 1, 0)
cancer_y_test_l <- ifelse(cancer_y_test =="M", 1, 0)
```

```{r  message=FALSE,  warning=FALSE}
glm_fit <- train(cancer_x_train,
                 cancer_y_train_l, 
                 method = "glm",
                 family = "binomial",
                 preProc = c("center", "scale"))
```

```{r }
# Predict on test data
glm_predict <- predict(glm_fit, newdata = cancer_x_test)

```

convert probability outcome into categorical outcome 
```{r }
y_pred_num <- ifelse(glm_predict > 0.5, 1, 0)
```

```{r }
# Print Confusion matrix, Accuarcy, Sensitivity etc 
confusionMatrix(as.factor(y_pred_num), as.factor(cancer_y_test_l), positive = "1")

# Add results into clf_results dataframe
x3 <- confusionMatrix(as.factor(y_pred_num), as.factor(cancer_y_test_l), positive = "1")[["overall"]]
y3 <- confusionMatrix(as.factor(y_pred_num), as.factor(cancer_y_test_l),positive = "1")[["byClass"]]

clf_results[nrow(clf_results) + 1,] <-  list(Model = "Logistic Regression", 
                                             Accuracy = round (x3[["Accuracy"]],3), 
                                            Precision = round (y3[["Precision"]],3), 
                                            Recall = round (y3[["Recall"]],3), 
                                            F1 = round (y3[["F1"]],3))

# Print Accuracy and F1 score
cat("Accuarcy is ", round(x3[["Accuracy"]],3), "and F1 is ", round (y3[["F1"]],3)  )

# Add results into cost_benefit_df dataframe for cost benefit analysis 
a3 <- confusionMatrix(as.factor(y_pred_num), as.factor(cancer_y_test_l))

#be careful about accurately pickign up the TP, FN, FP and TN
cost_benefit_df[nrow(cost_benefit_df) + 1,] <-  list(Model = "Logistic Regression", 
                                             TP = a3[["table"]][4], 
                                             FN = a3[["table"]][3], 
                                             FP = a3[["table"]][2], 
                                             TN = a3[["table"]][1])
```



## 1.5 XGBoost classification

```{r message=FALSE,  warning=FALSE}
XG_clf_fit <- train(cancer_x_train, 
                    cancer_y_train,
                    method = "xgbTree",
                    preProc = c("center", "scale"))
```

```{r }
# print the final model
XG_clf_fit$finalModel
```

```{r }
# Predict on test data
XG_clf_predict <- predict(XG_clf_fit,cancer_x_test)
```

```{r }
# Print Confusion matrix, Accuracy, Sensitivity etc 
confusionMatrix(XG_clf_predict,  cancer_y_test, positive = "M" )

# Add results into clf_results dataframe
x4 <- confusionMatrix(XG_clf_predict,  cancer_y_test )[["overall"]]
y4 <- confusionMatrix(XG_clf_predict,  cancer_y_test )[["byClass"]]

clf_results[nrow(clf_results) + 1,] <-  list(Model = "XG Boost", 
                                             Accuracy = round (x4[["Accuracy"]],3), 
                                            Precision = round (y4[["Precision"]],3), 
                                            Recall = round (y4[["Recall"]],3), 
                                            F1 = round (y4[["F1"]],3))

# Print Accuracy and F1 score
cat("Accuarcy is ", round(x4[["Accuracy"]],3), "and F1 is ", round (y4[["F1"]],3)  )

# Add results into cost_benefit_df dataframe for cost benefit analysis 
a4 <- confusionMatrix(XG_clf_predict,  cancer_y_test )

cost_benefit_df[nrow(cost_benefit_df) + 1,] <-  list(Model = "XG Boost", 
                                             TP = a4[["table"]][1], 
                                             FN = a4[["table"]][2], 
                                             FP = a4[["table"]][3], 
                                             TN = a4[["table"]][4])

```

## 1.6 Neural Network classification

```{r message=FALSE,  warning=FALSE }

# Try different combinations of parameters like 
# decay (prevents the weights from growing too large,) 
# and size of Hidden layers
my.grid <- expand.grid(.decay = c(0.5, 0.1), .size = c(5, 7))

# stepmax is maximum steps for the training of the neural network
# threshold is set to 0.01, meaning that if the change in error during an iteration is 
# less than 1%, then no further optimization will be carried out by the model
nn_clf_fit <- train(cancer_x_train,
                    cancer_y_train,
                    method = "nnet",
                    trace = F,
                    tuneGrid = my.grid,
                    linout = 0,
                    stepmax = 100,
                    threshold = 0.01 )
print(nn_clf_fit)

# Plot Neural Network 
plotnet(nn_clf_fit$finalModel, y_names = "Cancer Type")

```

```{r }
# Predict on test data
nn_clf_predict <- predict(nn_clf_fit,cancer_x_test)
```

Confusion matrix
```{r }
# Print Confusion matrix, Accuarcy, Sensitivity etc 
confusionMatrix(nn_clf_predict,  cancer_y_test, positive = "M")

# Add results into clf_results dataframe
x5 <- confusionMatrix(nn_clf_predict,  cancer_y_test)[["overall"]]
y5 <- confusionMatrix(nn_clf_predict,  cancer_y_test)[["byClass"]]

clf_results[nrow(clf_results) + 1,] <-  list(Model = "Neural Network", 
                                             Accuracy = round (x5[["Accuracy"]],3), 
                                            Precision = round (y5[["Precision"]],3), 
                                            Recall = round (y5[["Recall"]],3), 
                                            F1 = round (y5[["F1"]],3))

# Print Accuracy and F1 score
cat("Accuarcy is ", round(x5[["Accuracy"]],3), "and F1 is ", round (y5[["F1"]],3)  )


# Add results into cost_benefit_df dataframe for cost benefit analysis 
a5 <- confusionMatrix(nn_clf_predict,  cancer_y_test)

cost_benefit_df[nrow(cost_benefit_df) + 1,] <-  list(Model = "Neural Network", 
                                             TP = a5[["table"]][1], 
                                             FN = a5[["table"]][2], 
                                             FP = a5[["table"]][3], 
                                             TN = a5[["table"]][4])

```

**Compare Accuracy for all Classification models **

```{r }

print(clf_results)

# Plot accuracy for all the Classification Models

ggplot(clf_results %>% arrange(desc(Accuracy)) %>%
       mutate(Model=factor(Model, levels=Model) ), 
       aes(x = Model, y = Accuracy)) +
  geom_bar(stat = "identity" , width=0.3, fill="steelblue") + 
  coord_cartesian(ylim = c(0.88, 1)) +
  geom_hline(aes(yintercept = mean(Accuracy)),
             colour = "green",linetype="dashed") +
  ggtitle("Compare Accuracy for all Models") +
  theme(plot.title = element_text(color="black", size=10, hjust = 0.5))


```

## 1.7 Cost Benefit analysis

A model with high accuracy need not be the most profitable one. We can assign different costs to True Positive (TP), True Negative (TN), False Positive (FP), False Negative (FN) and evaluate each model and figure out which one is the most profitable model.

For this exercise lets assume that: 

benefit_TP = benefit for correctly predicting the cell type to be benign = $1000
benefit_TN = benefit for correctly predicting the cell type to be malignant = $4000 (so that you have a shot at curing it)
cost_FP = cost of incorrectly predicting a cancer cell as B= $5000 as it could lead to no further screening and eventual death
cost_FN= cost of incorrectly predicting a cancer cell as M= $200 (cost of additional test that would clarify the situation


```{r}

benefit_TP = 1000
benefit_TN = 4000
cost_FN = -200
cost_FP = -5000

cost_benefit_df <- cost_benefit_df %>% 
                    mutate(Profit = (benefit_TP * TP) + (benefit_TN * TN) + 
                                    (cost_FP * FP) + (cost_FN * FN))
```

**Compare Profit for all Classification models**

```{r}

print(cost_benefit_df)

# Plot Profit for all the Classification Models

ggplot(cost_benefit_df %>% arrange(desc(Profit)) %>%
       mutate(Model=factor(Model, levels=Model) ), 
       aes(x = Model, y = Profit)) +
  geom_bar(stat = "identity" , width=0.3, fill="steelblue") + 
  coord_cartesian(ylim = c(200000, 300000)) +
  geom_hline(aes(yintercept = mean(Profit)),
             colour = "green",linetype="dashed") +
  ggtitle("Compare Profit for all Models") +
  theme(plot.title = element_text(color="black", size=10, hjust = 0.5))

```

## 1.8 ROC and Lift curves for all models

ROC curve - It is a performance measurement for classification problem at various thresholds settings. It tells how much a model is capable of distinguishing between classes.

Y axis - True Positive rate or Sensitivity  = (TP / TP + FN)

X axis - False Positive rate or (1 - specificity) = (FP / TN + FP) 

AUC - Area under ROC curve. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s.

Lets Plot ROC curves for all the Models. The more "up and to the left" the ROC curve of a model is, the better the model. Also, higher the Area under curve, the better the model.

```{r}

# Predict probabilities of each model to plot ROC curve
knnPredict_prob <- predict(knn_clf_fit, newdata = cancer_x_test, type = "prob") 
dtree_prob <- predict(dtree_fit, newdata = cancer_x_test, type = "prob")
XG_boost_prob <- predict(XG_clf_fit, newdata = cancer_x_test, type = "prob")
nn_clf_prob <- predict(nn_clf_fit, newdata = cancer_x_test, type = "prob")

# List of predictions
preds_list <- list(knnPredict_prob[,2], dtree_prob[,2], 
                   glm_predict, XG_boost_prob[,2], nn_clf_prob[,2] )

# List of actual values (same for all)
m <- length(preds_list)
actuals_list <- rep(list(cancer_y_test_l), m)

# Plot the ROC curves
pred <- prediction(preds_list, actuals_list)
rocs <- performance(pred, "tpr", "fpr")

# calculate AUC for all models
AUC_models <- performance(pred, "auc")
auc_knn = round(AUC_models@y.values[[1]], 3)
auc_dt = round(AUC_models@y.values[[2]], 3)
auc_lr = round(AUC_models@y.values[[3]], 3)
auc_xg = round(AUC_models@y.values[[4]], 3)
auc_nn = round(AUC_models@y.values[[5]], 3)

# Plot the ROC curves
plot(rocs, col = as.list(1:m), main = "ROC Curves of different models")
legend(x = "bottomright", 
       legend = c( paste0("KNN - ", auc_knn), 
                   paste0("Decision Tree - ", auc_dt), 
                   paste0("Logistic Regression - ", auc_lr), 
                   paste0("XG Boost - ", auc_xg), 
                   paste0("Neural Net - ", auc_nn)), fill = 1:m)

```

**Lift curve** - Lift is a measure of the effectiveness of a predictive model calculated as the ratio between the results obtained with and without the predictive model. The lift chart shows how much more likely we are to predict the correct outcome than a random guess.


```{r}

lifts <- performance(pred, "lift", "rpp")

# Plot the Lift curves
plot(lifts, col = as.list(1:m), main = "Lift Curves of Different Models")
legend(x = "bottomleft", 
       legend = c( "KNN", 
                   "Decision Tree", 
                   "Logistic Regression", 
                   "XG Boost", 
                   "Neural Net"), fill = 1:m)


```

