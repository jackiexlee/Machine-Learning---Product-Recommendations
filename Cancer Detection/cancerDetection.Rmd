---
title: 'Cancer Detection - Predictive Analytics In-class'
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: 2
  pdf_document:
    toc: yes
    toc_depth: 2
---

\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE) #global setting in knitting to show code all through the knitting 
knitr::opts_knit$set(root.dir = "C:/Users/rbapna/Dropbox/NYUPredModelingCourse/_______Fal2024/labs/lab4- cancerDetection")
```


```{r message=FALSE,  warning=FALSE}
# load the required libraries
library("readxl") # used to read excel files
library("dplyr") # used for data munging 
library("FNN") # used for knn regression (knn.reg function)
library("caret") # used for various predictive models
library("class") # for using confusion matrix function
library("rpart.plot") # used to plot decision tree
library("rpart")  # used for Regression tree
library("glmnet") # used for Lasso and Ridge regression
library('NeuralNetTools') # used to plot Neural Networks
library("PRROC") # top plot ROC curve
library("ROCR") # top plot lift curve
library("e1071")
```


# 1. Classification


## 1.1 Data loading and transformation

Data Description: https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.names

```{r }
# Load the Breast Cancer data set

cancer_data = read.csv("wdbc.data", header = FALSE)
cancer_data$V2 <- as.factor(cancer_data$V2) # Tell R that V2 is the categorical outcome (M for malignant, B for benign)

# create Y and X data frames
cancer_y = cancer_data %>% pull("V2") # pulls out the outcome variable
# exclude V1 since its a row number
cancer_x = cancer_data %>% select(-c("V1", "V2")) #excludes V1 and V2 columns because V1 is row number and V2 is outcome variable
head(cancer_x)
```

Create a function that normalises columns since scale for each column might be different.

```{r }
# function to normalize data (0 to 1)
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}
```

```{r }
# Normalize x variables since they are at different scale
cancer_x_normalized <- as.data.frame(lapply(cancer_x, normalize)) 
#since we are calculating euclidean distance, we MUST normalize the data. lapply applies the function to each column of cancer_x
head(cancer_x_normalized)
```

Create Training and Testing data sets

```{r }
# 75% of the data is used for training and rest for testing
smp_size <- floor(0.75 * nrow(cancer_x_normalized))

# randomly select row numbers for training data set
#seq_len will take one argument and create a sequence from 1 to that argument
train_ind <- sample(seq_len(nrow(cancer_x_normalized)), size = smp_size)

# creating test and training sets for x
cancer_x_train <- cancer_x_normalized[train_ind, ]
cancer_x_test <- cancer_x_normalized[-train_ind, ]

# creating test and training sets for y
cancer_y_train <- cancer_y[train_ind]
cancer_y_test <- cancer_y[-train_ind]

# Create an empty data frame to store results from different models
clf_results <- data.frame(matrix(ncol = 5, nrow = 0))
names(clf_results) <- c("Model", "Accuracy", "Precision", "Recall", "F1")



```

**Cross validation**

It is a technique to use same training data but some portion of it for training and rest for validation of model. This technique reduces chances of overfitting

**Hyperparamter tuning**

We provide a list of hyperparameters to train the model. This helps in identifying best set of hyperparameters for a given model like Decision tree. **train** function in caret library automatically stores the information of the best model and its hyperparameters.



## 1.2 KNN Classification

```{r }

# ------------------------------------------------------------
# 1) Tell caret HOW to evaluate models: cross-validation plan
# ------------------------------------------------------------
cross_validation <- trainControl(
  method  = "repeatedcv",  # use cross-validation, but repeat it for more stable estimates
  number  = 10,            # 10 folds: split training data into 10 parts, rotate which part is "validation"
  repeats = 3              # do the whole 10-fold thing 3 times with different splits
)
# Net effect: every candidate model gets scored 30 times (10 folds × 3 repeats).
# This reduces “lucky split” randomness and gives a more reliable accuracy estimate.


# ------------------------------------------------------------
# 2) Tell caret WHICH hyperparameters to try
# ------------------------------------------------------------
# For k-NN, the key hyperparameter is k (how many neighbors to look at).
# expand.grid(k = 1:10) builds a small table of candidate k values: 1, 2, ..., 10.
Param_Grid <- expand.grid(k = 1:10)

# caret will train a separate model for EACH k in this grid,
# and evaluate each one using the cross_validation plan above.


# ------------------------------------------------------------
# 3) Train models + tune hyperparameters in one go
# ------------------------------------------------------------
knn_clf_fit <- train(cancer_x_train,   # your training features (columns with inputs)
  cancer_y_train,   # your training labels/targets (what you want to predict)
  method="knn",            # ask caret to use the k-nearest neighbors algorithm
  tuneGrid=Param_Grid,       # try k = 1..10
  trControl=cross_validation  # evaluate each k using repeated 10-fold CV
)
# What actually happens under the hood:
# - For k = 1, caret runs 10-fold CV, repeated 3 times, collects accuracy/Kappa.
# - For k = 2, same deal. ...
# - For k = 10, same deal.
# - Then it compares the average CV scores across all k values and picks the best one.
#   That “best k” is your tuned hyperparameter choice.


# ------------------------------------------------------------
# 4) Inspect results (which k won? how accurate was it?)
# ------------------------------------------------------------
knn_clf_fit
# Prints:
# - The resampling summary (mean Accuracy and Kappa for each k)
# - The selected best k (the one with the highest metric, usually Accuracy by default)
# - The final model trained on the FULL training set using that best k

```

```{r }
# Plot accuracies for different k values
plot(knn_clf_fit)

# print the best model
print(knn_clf_fit$finalModel)
```

```{r }
# Predict on test data
knnPredict <- predict(knn_clf_fit, newdata = cancer_x_test) 

```

```{r }
# ----------------------------------------------------------
# 1) Confusion Matrix + metrics for your model's predictions
# ----------------------------------------------------------
# knnPredict      = the labels predicted by your KNN model (e.g., "M" or "B")
# cancer_y_test   = the actual true labels for the test set
# positive = "M"  = tells R that the "M" class is the one we care about most 
#                   (e.g., malignant tumors if this is cancer data)
# mode = "prec_recall" makes sure metrics like Precision, Recall, F1 are available

confusionMatrix(knnPredict, cancer_y_test, positive = "M", mode = "prec_recall")

# This line will print:
# - Confusion matrix table: TP, FP, FN, TN counts
# - Accuracy: overall % correct
# - Precision: of all predicted M, how many were truly M
# - Recall (a.k.a. Sensitivity): of all actual M, how many did we find
# - F1 score: harmonic mean of Precision and Recall (balances both)


# ----------------------------------------------------------
# 2) Pull the metrics out to use later
# ----------------------------------------------------------
# [["overall"]]    = dictionary of overall stats like Accuracy
# [["byClass"]]    = dictionary of per-class stats like Precision, Recall, F1

x1 <- confusionMatrix(knnPredict, cancer_y_test, positive = "M")[["overall"]]
y1 <- confusionMatrix(knnPredict, cancer_y_test, positive = "M")[["byClass"]]


# ----------------------------------------------------------
# 3) Add metrics into your results dataframe
# ----------------------------------------------------------
# Assume clf_results is an existing table holding results for multiple models.
# We're adding a new row with KNN's metrics.

clf_results[nrow(clf_results) + 1,] <- list(
  Model     = "KNN",                           # name of the model
  Accuracy  = round(x1[["Accuracy"]], 3),      # accuracy, rounded to 3 decimals
  Precision = round(y1[["Precision"]], 3),     # precision
  Recall    = round(y1[["Recall"]], 3),        # recall
  F1        = round(y1[["F1"]], 3)             # F1 score
)


# ----------------------------------------------------------
# 4) Print just Accuracy and F1 nicely
# ----------------------------------------------------------
# cat() prints text + numbers together without quotes or brackets.

cat("Accuracy is", round(x1[["Accuracy"]], 3), "and F1 is", round(y1[["F1"]], 3))



```


## 1.3 Decision Tree Classification 

```{r }

# ------------------------------------------------------------
# Cross validation setup
# ------------------------------------------------------------
cross_validation <- trainControl(        # trainControl() tells R how to test each model
  method  = "repeatedcv",                # use repeated cross-validation
  number  = 10,                          # split data into 10 folds (10-fold CV)
  repeats = 3                            # repeat the 10-fold CV three times for stability
)
# Effect: each model will be tested on many different splits to avoid "lucky" or "unlucky" splits.


# ------------------------------------------------------------
# Hyperparameter tuning setup
# ------------------------------------------------------------
# maxdepth = maximum depth of the decision tree 
# (how many times you can split from the root to the deepest leaf)
Param_Grid <- expand.grid(maxdepth = 2:10)  
# expand.grid() creates a table of all values from 2 to 10 for maxdepth
# So we'll try trees with depth 2, depth 3, ... all the way to depth 10


# ------------------------------------------------------------
# See which hyperparameters are used by a given model
# ------------------------------------------------------------
modelLookup("rpart2")  
# This shows the tuning parameters for the 'rpart2' model 
# so you know 'maxdepth' is the right one to tune


# ------------------------------------------------------------
# Fit (train) the decision tree model
# ------------------------------------------------------------
dtree_fit <- train(
  cancer_x_train,                        # training data: input features
  cancer_y_train,                        # training data: output labels
  method = "rpart2",                     # use decision tree model rpart2
  parms = list(split = "gini"),          # splitting criterion = Gini index (common choice)
  tuneGrid = Param_Grid,                  # try all maxdepth values in Param_Grid
  trControl = cross_validation,          # use the cross-validation plan defined above
  preProc = c("center", "scale")          # standardize data (optional, trees don't really need it)
)
# This trains many decision trees, one for each maxdepth, 
# tests each one using repeated cross-validation, 
# then finds the best-performing depth.


# ------------------------------------------------------------
# Check accuracy results
# ------------------------------------------------------------
dtree_fit
# Prints a table of accuracy for each maxdepth and shows the best depth chosen

#Visualizing decision tree (trained at best maxdepth on all training data)
rpart.plot(dtree_fit$finalModel, type = 2, extra = 106, under = TRUE, faclen = 0)
# type/extra just make the plot readable: node labels, class probs, etc.
```

```{r }
# print the final model
dtree_fit$finalModel
```

```{r }
# Another way to plot decision tree
prp(dtree_fit$finalModel, box.palette = "Reds", tweak = 1.2)
```

```{r }
# Predict on test data
dtree_predict <- predict(dtree_fit, newdata = cancer_x_test)
```

```{r }
# Print Confusion matrix, Accuarcy, Sensitivity etc 
confusionMatrix(dtree_predict,  cancer_y_test, positive = "M", mode="prec_recall" )

# Add results into clf_results dataframe
x2 <- confusionMatrix(dtree_predict,  cancer_y_test, positive = "M" )[["overall"]]
y2 <- confusionMatrix(dtree_predict,  cancer_y_test, positive = "M" )[["byClass"]]

clf_results[nrow(clf_results) + 1,] <-  list(Model = "Decision Tree", 
                                             Accuracy = round (x2[["Accuracy"]],3), 
                                            Precision = round (y2[["Precision"]],3), 
                                            Recall = round (y2[["Recall"]],3), 
                                            F1 = round (y2[["F1"]],3))

# Print Accuracy and F1 score

cat("Accuarcy is ", round(x2[["Accuracy"]],3), "and F1 is ", round (y2[["F1"]],3)  )



```

## 1.4 Logistic regression


```{r  message=FALSE,  warning=FALSE}
glm_fit <- train(cancer_x_train,
                 cancer_y_train, 
                 method = "glm",
                 family = "binomial",
                 preProc = c("center", "scale"))
```

```{r }
# Predict on test data
glm_predict <- predict(glm_fit, newdata = cancer_x_test)
glm_predict_prob <- predict(glm_fit, newdata = cancer_x_test, type="prob")

```

convert probability outcome into categorical outcome 
```{r }
y_pred_num <- ifelse(glm_predict_prob[1] > 0.5, "B","M")
#column 1 is prob("B)
```

```{r }
# Print Confusion matrix, Accuarcy, Sensitivity etc 

confusionMatrix(as.factor(y_pred_num), as.factor(cancer_y_test), positive = "M", mode="prec_recall")

# Add results into clf_results dataframe
x3 <- confusionMatrix(as.factor(y_pred_num), as.factor(cancer_y_test), positive = "M")[["overall"]]
y3 <- confusionMatrix(as.factor(y_pred_num), as.factor(cancer_y_test), positive = "M")[["byClass"]]

clf_results[nrow(clf_results) + 1,] <-  list(Model = "Logistic Regression", 
                                             Accuracy = round (x3[["Accuracy"]],3), 
                                            Precision = round (y3[["Precision"]],3), 
                                            Recall = round (y3[["Recall"]],3), 
                                            F1 = round (y3[["F1"]],3))

# Print Accuracy and F1 score
cat("Accuarcy is ", round(x3[["Accuracy"]],3), "and F1 is ", round (y3[["F1"]],3)  )


```


**Compare Accuracy for all Classification models **

```{r }

print(clf_results)

# Plot accuracy for all the Classification Models

ggplot(clf_results %>% 
         arrange(desc(Accuracy)) %>% 
         mutate(Model = factor(Model, levels = unique(Model))),
       aes(x = Model, y = Accuracy)) +
  geom_col(width = 0.3, fill = "steelblue") +
  coord_cartesian(ylim = c(0.88, 1)) +
  geom_hline(yintercept = mean(clf_results$Accuracy),
             colour = "green", linetype = "dashed") +
  ggtitle("Compare Accuracy for all Models") +
  theme(plot.title = element_text(color="black", size=10, hjust=0.5))



```
