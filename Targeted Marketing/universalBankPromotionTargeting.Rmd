---
title: "Universal Bank - Promotion Targeting"
date: "10/15/2020"
output:
  html_document:
    toc: true
    toc_depth: 2
    df_print: paged
  pdf_document:
    toc: yes
    toc_depth: 2
---
\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "C:\\Users\\rbapna\\Dropbox\\NYUPredModelingCourse\\labs\\lab5 - PromotionTargetingUniversalBank")

```


```{r message=FALSE,  warning=FALSE}
# load the required libraries
library("readxl") # used to read excel files
library("dplyr") # used for data munging 
library("FNN") # used for knn regression (knn.reg function)
library("caret") # used for various predictive models
library("class") # for using confusion matrix function
library("rpart.plot") # used to plot decision tree
library("rpart")  # used for Regression tree
library("glmnet") # used for Lasso and Ridge regression
library('NeuralNetTools') # used to plot Neural Networks
library("PRROC") # top plot ROC curve
library("ROCR") # top plot lift curve
library("skimr")
```


# 1. Classification


## 1.1 Data loading and transformation

The Excel file has three sheets. There is the data dictionary, 2000 rows of past data to train a model, and 500 rows of new (unlabeled) data to 
identify which top 50 people to target. 

```{r }
# Load the Universal Bank  data set

bank1 <- read_excel("CL-bank-training-testingPredicting.xlsx", 
                    sheet = 2, 
                    col_types = c("numeric", "numeric", "numeric", 
                                  "numeric", "skip", "numeric", "numeric", 
                                  "numeric", "numeric", "numeric", 
                                  "numeric", "numeric", "numeric", 
                                  "text"))

skim(bank1)

#notice, we imported the Outcome "PersonalLoan" column as text. 
#this will make it easier to convert the y column to be of the type factor, which tells R to do binary classifaction 
#you have to be careful if you import an outcome that is 0 or 1 and you intend it to be factor
bank1$PersonalLoan <- as.factor(bank1$PersonalLoan)

# create Y and X data frames
#we will need the y column as a vector (X to be a dataframe)
# dplyr allows us to do this by using 'pull' instead of select
bank1_y = bank1 %>% pull("PersonalLoan") 

# exclude column 1 since its a ID 
bank1_x = bank1 %>% select(-c("ID", "PersonalLoan"))
```

Create a function that normalizes columns since scale for each column might be different.

```{r }
# function to normalize data (0 to 1)
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}
```

```{r }
# Normalize x variables since they are at different scale
bank1_x_normalized <- as.data.frame(lapply(bank1_x, normalize))
```

Create Training and Testing data sets

```{r }
# 75% of the data is used for training and rest for testing
smp_size <- floor(0.75 * nrow(bank1_x_normalized))

# randomly select row numbers for training data set
set.seed(12345)
train_ind <- sample(seq_len(nrow(bank1_x_normalized)), size = smp_size)

# creating test and training sets for x
bank1_x_train <- bank1_x_normalized[train_ind, ]
bank1_x_test <- bank1_x_normalized[-train_ind, ]

# creating test and training sets for y
bank1_y_train <- bank1_y[train_ind]
bank1_y_test <- bank1_y[-train_ind]

# Create an empty data frame to store results from different models
clf_results <- data.frame(matrix(ncol = 5, nrow = 0))
names(clf_results) <- c("Model", "Accuracy", "Precision", "Recall", "F1")

# Create an empty data frame to store TP, TN, FP and FN values
cost_benefit_df <- data.frame(matrix(ncol = 5, nrow = 0))
names(cost_benefit_df) <- c("Model", "TP", "FN", "FP", "TN")


```

**Cross validation**

It is a technique to use same training data but some portion of it for training and rest for validation of model. This technique reduces chances of overfitting

**Hyperparamter tuning**

We provide a list of hyperparameters to train the model. This helps in identifying best set of hyperparameters for a given model like Decision tree. **train** function in caret library automatically stores the information of the best model and its hyperparameters.



## 1.2 KNN Classification

```{r }

# Cross validation 
cross_validation <- trainControl(## 10-fold CV
                                method = "repeatedcv",
                                number = 10,
                                ## repeated three times
                                repeats = 3)
# Hyperparamter tuning
# k = number of nearest neighbors
Param_Grid <-  expand.grid( k = 1:10)

#for every k from 1: 10 it will do 10 fold cross validation, 3 times

# fit the model to training data
knn_clf_fit <- train(bank1_x_train,
                     bank1_y_train,
                     method = "knn",
                     tuneGrid = Param_Grid,
                     trControl = cross_validation )


#you can see the sampling process
knn_clf_fit$resample %>% arrange(Resample)


# check the accuracy for different models
knn_clf_fit

```

```{r }
# Plot accuracies for different k values
plot(knn_clf_fit)

# print the best model
print(knn_clf_fit$finalModel)
```

```{r }
# Predict on test data
knnPredict <- predict(knn_clf_fit, newdata = bank1_x_test) 

```

```{r }
# Print Confusion matrix, Accuarcy, Sensitivity etc 
confusionMatrix(knnPredict, bank1_y_test, positive="1", mode="prec_recall")

# Add results into clf_results dataframe
x1 <- confusionMatrix(knnPredict, bank1_y_test, positive="1")[["overall"]]
y1 <- confusionMatrix(knnPredict, bank1_y_test, positive="1")[["byClass"]]

clf_results[nrow(clf_results) + 1,] <-  list(Model = "KNN", 
                                             Accuracy = round (x1[["Accuracy"]],3), 
                                            Precision = round (y1[["Precision"]],3), 
                                            Recall = round (y1[["Recall"]],3), 
                                            F1 = round (y1[["F1"]],3))
# Print Accuracy and F1 score

cat("Accuarcy is ", round(x1[["Accuracy"]],3), "and F1 is ", round (y1[["F1"]],3)  )

# Add results into cost_benefit_df dataframe for cost benefit analysis 
a1 <- confusionMatrix(knnPredict, bank1_y_test)

cost_benefit_df[nrow(cost_benefit_df) + 1,] <-  list(Model = "KNN", 
                                             TP = a1[["table"]][4], 
                                             FN = a1[["table"]][3], 
                                             FP = a1[["table"]][2], 
                                             TN = a1[["table"]][1])

```


## 1.3 Decision Tree Classification 

```{r }

# Cross validation
cross_validation <- trainControl(## 10-fold CV
                                method = "repeatedcv",
                                number = 10,
                                ## repeated three times
                                repeats = 3)
# Hyperparamter tuning
# maxdepth =  the maximum depth of the tree that will be created or
# the length of the longest path from the tree root to a leaf.


#lets see what hyper parameters are needed /available to tune the decision tree using rpart2 package
modelLookup("rpart2")
Param_Grid <-  expand.grid(maxdepth = 2:10)


dtree_fit <- train(bank1_x_train,
                   bank1_y_train, 
                   method = "rpart2",
                   # split - criteria to split nodes
                   parms = list(split = "gini"),
                  tuneGrid = Param_Grid,
                   trControl = cross_validation,
                  # preProc -  perform listed pre-processing to predictor dataframe
                   preProc = c("center", "scale"))

# check the accuracy for different models
dtree_fit

dtree_fit$resample
```

```{r }
# print the final model
dtree_fit$finalModel
```

```{r }
# Plot decision tree
prp(dtree_fit$finalModel, box.palette = "Reds", tweak = 1.2)
```

```{r }
# Predict on test data
dtree_predict <- predict(dtree_fit, newdata = bank1_x_test)
dtree_predict_prob <- predict(dtree_fit, newdata = bank1_x_test, type = "prob")
```

```{r }
# Print Confusion matrix, Accuarcy, Sensitivity etc 
confusionMatrix(dtree_predict,  bank1_y_test, positive="1", mode="prec_recall" )

# Add results into clf_results dataframe
x2 <- confusionMatrix(dtree_predict,  bank1_y_test, positive="1")[["overall"]]
y2 <- confusionMatrix(dtree_predict,  bank1_y_test, positive="1" )[["byClass"]]

clf_results[nrow(clf_results) + 1,] <-  list(Model = "Decision Tree", 
                                             Accuracy = round (x2[["Accuracy"]],3), 
                                            Precision = round (y2[["Precision"]],3), 
                                            Recall = round (y2[["Recall"]],3), 
                                            F1 = round (y2[["F1"]],3))

# Print Accuracy and F1 score

cat("Accuarcy is ", round(x2[["Accuracy"]],3), "and F1 is ", round (y2[["F1"]],3)  )

# Add results into cost_benefit_df dataframe for cost benefit analysis 
a2 <- confusionMatrix(dtree_predict,  bank1_y_test )

cost_benefit_df[nrow(cost_benefit_df) + 1,] <-  list(Model = "Decision Tree", 
                                             TP = a2[["table"]][4], 
                                             FN = a2[["table"]][3], 
                                             FP = a2[["table"]][2], 
                                             TN = a2[["table"]][1])

```

## 1.4 Logistic regression

Convert categorical outcome into numerical. Logistic regression cannot handle categorical variables


```{r  message=FALSE,  warning=FALSE}
glm_fit <- train(bank1_x_train,
                 bank1_y_train, 
                 method = "glm",
                 family = "binomial",
                 preProc = c("center", "scale"))

glm_fit$resample
#notice that the model was estimated 25 times
glm_fit$resampledCM
```

```{r }
# Predict on test data
glm_predict <- predict(glm_fit, newdata = bank1_x_test)
glm_predict_prob <- predict(glm_fit, newdata = bank1_x_test, type="prob")

```

convert probability outcome into categorical outcome 
```{r }
y_pred_num <- ifelse(glm_predict_prob[,2] > 0.4, 1, 0)
```

```{r }
# Print Confusion matrix, Accuarcy, Sensitivity etc 
confusionMatrix(as.factor(y_pred_num), as.factor(bank1_y_test), positive = "1", mode="prec_recall")

# Add results into clf_results dataframe
x3 <- confusionMatrix(as.factor(y_pred_num), as.factor(bank1_y_test), positive = "1")[["overall"]]
y3 <- confusionMatrix(as.factor(y_pred_num), as.factor(bank1_y_test), positive = "1")[["byClass"]]

clf_results[nrow(clf_results) + 1,] <-  list(Model = "Logistic Regression", 
                                             Accuracy = round (x3[["Accuracy"]],3), 
                                            Precision = round (y3[["Precision"]],3), 
                                            Recall = round (y3[["Recall"]],3), 
                                            F1 = round (y3[["F1"]],3))

# Print Accuracy and F1 score
cat("Accuarcy is ", round(x3[["Accuracy"]],3), "and F1 is ", round (y3[["F1"]],3)  )

# Add results into cost_benefit_df dataframe for cost benefit analysis 
a3 <- confusionMatrix(as.factor(y_pred_num), as.factor(bank1_y_test))

cost_benefit_df[nrow(cost_benefit_df) + 1,] <-  list(Model = "Logistic Regression", 
                                             TP = a3[["table"]][4], 
                                             FN = a3[["table"]][3], 
                                             FP = a3[["table"]][2], 
                                             TN = a3[["table"]][1])
```


**Compare Accuracy for all Classification models **

```{r }

print(clf_results)

# Plot accuracy for all the Classification Models

ggplot(clf_results[1:3,] %>% arrange(desc(Accuracy)) %>%
       mutate(Model=factor(Model, levels=Model) ), 
       aes(x = Model, y = Accuracy)) +
  geom_bar(stat = "identity" , width=0.3, fill="steelblue") + 
  coord_cartesian(ylim = c(0.88, 1)) +
  geom_hline(aes(yintercept = mean(Accuracy)),
             colour = "green",linetype="dashed") +
  ggtitle("Compare Accuracy for all Models") +
  theme(plot.title = element_text(color="black", size=10, hjust = 0.5))


```

## 1.7 Cost Benefit analysis

A model with high accuracy need not be the most profitable one. We can assign different costs to True Positive (TP), True Negative (TN), False Positive (FP), False Negative (FN) and evaluate each model and figure out which one is the most profitable model.

For this exercise lets assume that 

benefit of a true Positive is that you acquire a customer = 10000
benefit of a true negative is that you don't target a customer, and maybe save some operational costs and even annoyance costs to cust. = 10
cost of a false negative is that you missed out on acquiring this customer = -8000
cost of a false positive is that you target and potentially annoy this customer = -8000 = -100

```{r}

benefit_TP = 10000
benefit_TN = 10
cost_FN = -8000
cost_FP = -100

cost_benefit_df <- cost_benefit_df %>% 
                    mutate(Profit = (benefit_TP * TP) + (benefit_TN * TN) + 
                                    (cost_FP * FP) + (cost_FN * FN))
```

**Compare Profit for all Classification models**

```{r}

print(cost_benefit_df)

# Plot Profit for all the Classification Models

ggplot(cost_benefit_df[1:3,] %>% arrange(desc(Profit)) %>%
       mutate(Model=factor(Model, levels=Model) ), 
       aes(x = Model, y = Profit)) +
  geom_bar(stat = "identity" , width=0.3, fill="steelblue") + 
  coord_cartesian(ylim = c(100000, 500000)) +
    geom_hline(aes(yintercept = mean(Profit)),
             colour = "green",linetype="dashed") +
  ggtitle("Compare Profit for all Models") +
  theme(plot.title = element_text(color="black", size=10, hjust = 0.5))

```

#2 Scoring the target list 
##2.1 Let us use the decision tree model (as it had highest accuracy)

```{r}

# #load new data that has not been scored

bank1.new <- read_excel("CL-bank-training-testingPredicting.xlsx", 
                    sheet = 3, 
                    col_types = c("numeric", "numeric", "numeric", 
                                  "numeric", "skip", "numeric", "numeric", 
                                  "numeric", "numeric", "numeric", 
                                  "numeric", "numeric", "numeric", 
                                  "text"))

bank1.new_x <- bank1.new %>% select(-c("ID", "PersonalLoan"))

# Normalize x variables since they are at different scale
bank1.new_x_normalized <- as.data.frame(lapply(bank1.new_x, normalize))

#use the fitted tree model to score this new data
#note here, i want to rank order my targets by probability of responding
#hence we tell R type="prob" as opposed to type="class" 
#latter would give 0, 1 output



dTreePredict.new <- predict(dtree_fit, newdata = bank1.new_x_normalized, type="prob") 

#above results in 2 columns, probability of class 0 and probability of class 1 in the 2nd column
#we will extract the 2nd column

bank1.new$ProbAccepting <- dTreePredict.new[,2]

#sort this in descending order and add a column to the bank1.new data frame
#could have also used mutate here 

bank1.new <- bank1.new %>% arrange(desc(ProbAccepting))

#lets count 

successNum <-  bank1.new[1:50,] %>% filter(ProbAccepting>0.5) %>% tally()
# Print targeting accuracy 

cat("If we target the top 50 people ", successNum[[1]], " out of 50 have a chance > 50% of converting","\n") 
cat("If we have no model, we get a 10% repsonse rate and should get 5 hits")

#write this sorted target list as a CSV file
write.csv(x=bank1.new, file = "targetList1.csv", row.names = TRUE)


```
